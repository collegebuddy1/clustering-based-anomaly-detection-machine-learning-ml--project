{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear = lambda:os.system('clear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet():# Getting the path of the dataset\n",
    "    \n",
    "    while True:\n",
    "        print(\"**************************************************\")\n",
    "        print(\"DATA SET MENU\")\n",
    "        print(\"**************************************************\")\n",
    "        print(\"1.NSL-KDD\")\n",
    "        print(\"2.IDS 2017\")\n",
    "        \n",
    "        option = input(\"Option:\")\n",
    "        \n",
    "        if option == \"1\" or option == \"2\":\n",
    "            break\n",
    "    \n",
    "    path = input(\"Path of the File:\")\n",
    "    \n",
    "    return path,option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readingData(path): #Reading the Dataset\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        option = input(\"Dataset has feature names[y/n]:\") \n",
    "        \n",
    "        if option == \"y\" or option == \"n\":\n",
    "            break\n",
    "            \n",
    "    print(\"\\nReading Dataset...\") \n",
    "        \n",
    "    if option == \"y\":\n",
    "        dataSet = pd.read_csv(path,low_memory=False)\n",
    "    \n",
    "    elif option == \"n\":\n",
    "        dataSet = pd.read_csv(path, header = None,low_memory=False)\n",
    "            \n",
    "    return dataSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkMissing(X):#This check if the dataset given has missing values.\n",
    "    isMissing = str(X.isnull().values.any()) #Using String instead of Boolean because (\"cannot unpack non-iterable numpy.bool object\")\n",
    "    \n",
    "    if isMissing == \"True\":\n",
    "        #if data set has infinity values replace them with none\n",
    "        X = X.replace('Infinity', np.nan) #Replacing Infinity values with nan values\n",
    "           \n",
    "        missingValIndex = []\n",
    "        total = X.isnull().sum().sum()\n",
    "        percent = (total / (X.count().sum() + X.isnull().sum().sum())) * 100\n",
    "            \n",
    "        for rows in X:\n",
    "                    \n",
    "            if X[rows].isnull().sum() != 0:\n",
    "                missingValIndex.append(rows)\n",
    "        print(\"\\n\\n**************************************************\")\n",
    "        print(\"Data has missing values\")\n",
    "        print(\"**************************************************\")\n",
    "        print(\"Features with missing values:\",missingValIndex)\n",
    "        print(\"Total missing Values -> \" , total)\n",
    "        print(percent,\"%\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    for rows in dataSet: #Getting features index with missing values\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "#Getting The data we want to test for the clustering algorithms\n",
    "def gettingVariables(dataSet,dataSetOption):# If the dataset is NSL-KDD it would get the features and the labels for it and if its IDS 2017 it would take the features and the labels for it and take careof missing values.\n",
    "   \n",
    "    if dataSetOption == \"1\":\n",
    "        while True:\n",
    "            print(\"\\n\\n**************************************************\")\n",
    "            print(\"Variables Menu\")\n",
    "            print(\"**************************************************\")\n",
    "            print(\"1.Data set with categorical data oneHot encoded\")\n",
    "            print(\"2.Data set with categorical data removed\")\n",
    "            print(\"3.Data set with Risk Values replacing Server Type and Flag Features; Protocol Data oneHot encoded\")\n",
    "            option = input(\"Enter option :\")\n",
    "            \n",
    "            \n",
    "            if option == \"1\" or option == \"2\" or option == \"3\":\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "                print(\"Error\\n\\n\")\n",
    "            \n",
    "        \n",
    "        if option == \"1\":\n",
    "            #Getting the Dependent and independent Variables\n",
    "            #In all the option we remove the dificulty level feature because we don't need it in our experiments\n",
    "            \n",
    "            \n",
    "            X = dataSet.iloc[:,:-2].values # Data, Get all the rows and all the clums except all the colums - 2\n",
    "            Y = dataSet.iloc[:,42].values# Labels\n",
    "            return X,Y,option\n",
    "        \n",
    "        elif option == \"2\":\n",
    "            #Removing Categorical data from the data set\n",
    "            X = dataSet.iloc[:,[0,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]].values\n",
    "            Y = dataSet.iloc[:,42].values# Labels\n",
    "            \n",
    "            return X,Y,option\n",
    "        \n",
    "        elif option == \"3\":\n",
    "            #for later Risk Encode - Categorical features\n",
    "            X = dataSet.iloc[:,:-2].values\n",
    "            Y = dataSet.iloc[:,42].values# Labels\n",
    "            \n",
    "            return X,Y,option\n",
    "    \n",
    "\n",
    "    elif dataSetOption == \"2\":\n",
    "        #############################################################################\n",
    "        #GETTING VARIABLES\n",
    "        #############################################################################\n",
    "        missingValIndex = []\n",
    "        for rows in dataSet: #Getting features index with missing values\n",
    "            if dataSet[rows].isnull().sum() != 0:\n",
    "                    missingValIndex.append(rows)\n",
    "                \n",
    "        X = dataSet.iloc[:,:-1].values#data\n",
    "        #if names are not especified it will assign 0,1,2...n for the features name\n",
    "        X = pd.DataFrame(X,columns = [' Destination Port',' Flow Duration',' Total Fwd Packets',' Total Backward Packets','Total Length of Fwd Packets',\n",
    "                                      ' Total Length of Bwd Packets',' Fwd Packet Length Max',' Fwd Packet Length Min',' Fwd Packet Length Mean',' Fwd Packet Length Std',\n",
    "                                      'Bwd Packet Length Max',' Bwd Packet Length Min',' Bwd Packet Length Mean',' Bwd Packet Length Std','Flow Bytes/s',' Flow Packets/s',' Flow IAT Mean',\n",
    "                                      ' Flow IAT Std',' Flow IAT Max',' Flow IAT Min','Fwd IAT Total',' Fwd IAT Mean',' Fwd IAT Std',' Fwd IAT Max',' Fwd IAT Min','Bwd IAT Total',' Bwd IAT Mean',\n",
    "                                      ' Bwd IAT Std',' Bwd IAT Max',' Bwd IAT Min','Fwd PSH Flags',' Bwd PSH Flags',' Fwd URG Flags',' Bwd URG Flags',' Fwd Header Length',' Bwd Header Length','Fwd Packets/s',\n",
    "                                      ' Bwd Packets/s',' Min Packet Length',' Max Packet Length',' Packet Length Mean',' Packet Length Std',' Packet Length Variance','FIN Flag Count',' SYN Flag Count',' RST Flag Count',\n",
    "                                      ' PSH Flag Count',' ACK Flag Count',' URG Flag Count',' CWE Flag Count',' ECE Flag Count',' Down/Up Ratio',' Average Packet Size',' Avg Fwd Segment Size',' Avg Bwd Segment Size',' Fwd Header Length',\n",
    "                                      'Fwd Avg Bytes/Bulk',' Fwd Avg Packets/Bulk',' Fwd Avg Bulk Rate',' Bwd Avg Bytes/Bulk',' Bwd Avg Packets/Bulk','Bwd Avg Bulk Rate','Subflow Fwd Packets',' Subflow Fwd Bytes',' Subflow Bwd Packets',' Subflow Bwd Bytes',\n",
    "                                      'Init_Win_bytes_forward',' Init_Win_bytes_backward',' act_data_pkt_fwd',' min_seg_size_forward','Active Mean',' Active Std',' Active Max',' Active Min','Idle Mean',' Idle Std',' Idle Max',' Idle Min'])\n",
    "        Y = dataSet.iloc[:,78].values#Labels\n",
    "        \n",
    "        #############################################################################\n",
    "        #Variables Got \n",
    "        #############################################################################\n",
    "        \n",
    "    #############################################################################\n",
    "    #MANAGE MISSING DATA\n",
    "    #############################################################################   \n",
    "     \n",
    "        while True:\n",
    "            print(\"\\n\\n**************************************************\")\n",
    "            print(\"Manage Missing Values \")\n",
    "            print(\"**************************************************\")\n",
    "            print(\"1.Eliminate Catg. w/ Missing Values\")\n",
    "            print(\"2.Impute 0 for Missing Values\")\n",
    "            print(\"3.Impute Mean for Missing Values\")\n",
    "            print(\"4.Impute Median for Missing Values\")\n",
    "            print(\"5.Impute Mode for Missing Values\")\n",
    "            print(\"6.Simple Imputer\")\n",
    "            missingDataOption = input(\"Option:\")\n",
    "    \n",
    "            if missingDataOption == \"1\" or missingDataOption == \"2\" or missingDataOption == \"3\" or missingDataOption == \"4\" or missingDataOption == \"5\" or missingDataOption == \"6\":\n",
    "                break\n",
    "    \n",
    "    \n",
    "        if missingDataOption == \"1\":\n",
    "            deletedColumns = []\n",
    "            numColumns = len(X.columns)\n",
    "            #removing features with missing values\n",
    "            for row in missingValIndex:\n",
    "                deletedColumns.append(row)\n",
    "                del X[row]\n",
    "        \n",
    "            print(\"#\\n\\n########################################################################\")\n",
    "            print(\"Columns Succesfully Removed\")\n",
    "            print(len(deletedColumns),\"of\",numColumns,\"were deleted\")\n",
    "            print(\"Columns Names -> \",deletedColumns)\n",
    "            print(\"#########################################################################\")\n",
    "    \n",
    "        elif missingDataOption == \"2\":\n",
    "            #fill with 0\n",
    "            for row in missingValIndex:\n",
    "                X[row] = X[row].fillna(0)\n",
    "        \n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Sucessfully Filled Missing Values with 0\")\n",
    "            print(\"#########################################################################\")\n",
    "    \n",
    "    \n",
    "        elif missingDataOption == \"3\":\n",
    "            #mean imputer\n",
    "            for row in missingValIndex:\n",
    "                X[row] = X[row].astype(float)\n",
    "                X[row] = X[row].fillna(X[row].mean())\n",
    "        \n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Sucessfully Filled Missing Values with Mean\")\n",
    "            print(\"#########################################################################\")\n",
    "    \n",
    "        elif missingDataOption == \"4\":\n",
    "            #median imputer\n",
    "            for row in missingValIndex:\n",
    "                median = X[row].median()\n",
    "                X[row].fillna(median, inplace=True)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Sucessfully Filled Missing Values with Median\")\n",
    "            print(\"#########################################################################\")\n",
    "    \n",
    "        elif missingDataOption == \"5\":\n",
    "            #Mode imputer\n",
    "            for row in missingValIndex:\n",
    "                X[row] = X[row].fillna(X[row].mode()[0])\n",
    "    \n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Sucessfully Filled Missing Values with Mode \")\n",
    "            print(\"#########################################################################\")\n",
    "        \n",
    "        elif missingDataOption == \"6\": \n",
    "            from sklearn.impute import SimpleImputer\n",
    "            #\"Imputation transformer for completing missing values.\"(Univariate)\n",
    "            X = SimpleImputer(missing_values = np.nan, strategy='mean', fill_value=None, verbose=0, copy=True).fit_transform(X)          \n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Sucessfully Imputed Simple Imputer \")\n",
    "            print(\"#########################################################################\")\n",
    "                  \n",
    "                  \n",
    "        option = \"None\" #This data does not have categorical features so dataOption is none      \n",
    "        return X,Y,option\n",
    "       \n",
    "#############################################################################\n",
    "#END OF MISSING DATA\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodingLabels(Y,dataOption,datasetOption):# Encoding the labels with multi class or binary\n",
    "    \n",
    "    if datasetOption == \"1\": #Check if the data set choosen is NSL-KDD or IDS2017\n",
    "        \n",
    "        if dataOption == \"1\" or dataOption == \"2\" or dataOption == \"3\":\n",
    "            \n",
    "            while True:\n",
    "                print(\"\\n\\n#########################################################################\")\n",
    "                print(\"Encoding Menu\")\n",
    "                print(\"#########################################################################\")\n",
    "                print(\"1.Binary true labels: normal = 0, abnormal = 1\")\n",
    "                print(\"2.Multiclass true labels: normal = 0, DoS = 1, Probe = 2, R2L = 3, U2R = 4\")\n",
    "                encodeOption = input(\"Enter option :\") \n",
    "    \n",
    "                if encodeOption == \"1\" or encodeOption == \"2\":\n",
    "                    break\n",
    "                else:\n",
    "                    \n",
    "                    print(\"Error\\n\\n\")\n",
    "    \n",
    "    \n",
    "            if encodeOption == \"1\":\n",
    "                #Binary Categories\n",
    "                attackType  = {'normal':\"normal\", 'neptune':\"abnormal\", 'warezclient':\"abnormal\", 'ipsweep':\"abnormal\",'back':\"abnormal\", 'smurf':\"abnormal\", 'rootkit':\"abnormal\",'satan':\"abnormal\", 'guess_passwd':\"abnormal\",'portsweep':\"abnormal\",'teardrop':\"abnormal\",'nmap':\"abnormal\",'pod':\"abnormal\",'ftp_write':\"abnormal\",'multihop':\"abnormal\",'buffer_overflow':\"abnormal\",'imap':\"abnormal\",'warezmaster':\"abnormal\",'phf':\"abnormal\",'land':\"abnormal\",'loadmodule':\"abnormal\",'spy':\"abnormal\",'perl':\"abnormal\"} \n",
    "                attackEncodingCluster  = {'normal':0,'abnormal':1}\n",
    "    \n",
    "                Y[:] = [attackType[item] for item in Y[:]] #Encoding the binary data\n",
    "                Y[:] = [attackEncodingCluster[item] for item in Y[:]]#Changing the names of the labels to binary labels normal and abnormal\n",
    "                return Y,encodeOption\n",
    "    \n",
    "            elif encodeOption == \"2\":\n",
    "                #4 Main Categories\n",
    "                #normal = 0\n",
    "                #DoS = 1\n",
    "                #Probe = 2\n",
    "                #R2L = 3\n",
    "                #U2R = 4\n",
    "                attackType  = {'normal': 'normal', 'neptune':'DoS', 'warezclient': 'R2L', 'ipsweep': 'Probe','back': 'DoS', 'smurf': 'DoS', 'rootkit': 'U2R','satan': 'Probe', 'guess_passwd': 'R2L','portsweep': 'Probe','teardrop': 'DoS','nmap': 'Probe','pod': 'DoS','ftp_write': 'R2L','multihop': 'R2L','buffer_overflow': 'U2R','imap': 'R2L','warezmaster': 'R2L','phf': 'R2L','land': 'DoS','loadmodule': 'U2R','spy': 'R2L','perl': 'U2R'} \n",
    "                attackEncodingCluster  = {'normal':0,'DoS':1,'Probe':2,'R2L':3, 'U2R':4} #Main Categories\n",
    "    \n",
    "                Y[:] = [attackType[item] for item in Y[:]] #Encoding the main 4 categories\n",
    "                Y[:] = [attackEncodingCluster[item] for item in Y[:]]# Changing the names of attacks into 4 main categories\n",
    "                return Y,encodeOption\n",
    "        else:\n",
    "            return Y\n",
    "    \n",
    "    \n",
    "    elif datasetOption == \"2\":#Check if the data set choosen is NSL-KDD or IDS2017\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Encoding Menu\")\n",
    "        print(\"#########################################################################\")\n",
    "        print(\"1.Binary true labels: normal = 0, abnormal = 1\")\n",
    "        print(\"2. Multiclass true labels: BENIGN= 0, DoS slowloris= 1, DoS Slowhttptest= 2, DoS Hulk= 3, DoS GoldenEye= 4, Heartbleed= 5\")\n",
    "        encodeOption = input(\"Enter option :\")\n",
    "\n",
    "        if encodeOption == \"1\":\n",
    "            Y = np.array(Y,dtype= object)\n",
    "            attackEncoding  = {'BENIGN': 0,'DoS slowloris': 1,'DoS Slowhttptest': 2,'DoS Hulk': 3, 'DoS GoldenEye': 4, 'Heartbleed': 5} #Main Categories\n",
    "            Y[:] = [attackEncoding[item] for item in Y[:]]# Changing the names of attacks into 4 main categories\n",
    "    \n",
    "            return Y,encodeOption\n",
    "        \n",
    "        elif encodeOption == \"2\":\n",
    "            Y = np.array(Y,dtype= object)\n",
    "            attackType  = {'BENIGN': 'normal','DoS slowloris': 'abnormal','DoS Slowhttptest': 'abnormal','DoS Hulk': 'abnormal', 'DoS GoldenEye': 'abnormal', 'Heartbleed': 'abnormal'} #Binary Categories\n",
    "            attackEncoding = {'normal': 0, 'abnormal': 1}\n",
    "            \n",
    "            Y[:] = [attackType[item] for item in Y[:]]# Changing the names of attacks into binary categories\n",
    "            Y[:] = [attackEncoding[item] for item in Y[:]]# Changing the names of attacks into binary categories\n",
    "            return Y,encodeOption\n",
    "        \n",
    "        else:\n",
    "            return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the categorical features using one hot encoding and using Main attacks categories or binary categories\n",
    "def oneHotEncodingData(X,dataOption):\n",
    "        \n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    #We use One hot encoding to pervent the machine learning to atribute the categorical data in order. \n",
    "    #What one hot encoding(ColumnTransformer) does is, it takes a column which has categorical data, \n",
    "    #which has been label encoded, and then splits the column into multiple columns.\n",
    "    #The numbers are replaced by 1s and 0s, depending on which column has what value\n",
    "    #We don't need to do a label encoded step because ColumnTransformer do one hot encode and label encode!\n",
    "    #Encoding the Independient Variable\n",
    "    if dataOption == \"1\": #Only for dataset with Categorical Data\n",
    "        transform = ColumnTransformer([(\"Servers\", OneHotEncoder(categories = \"auto\"), [1,2,3])], remainder=\"passthrough\")\n",
    "        X = transform.fit_transform(X)\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Data has been successfully One Hot Encoded\")\n",
    "        print(\"#########################################################################\")\n",
    "\n",
    "        return X\n",
    "    elif dataOption == \"3\": #Only for risk data, because we don't have risk values for protocol feature we do one hot encoding for only that feature and the other ones we do risk value encoding\n",
    "        transform = ColumnTransformer([(\"Servers\", OneHotEncoder(categories = \"auto\"), [1])], remainder=\"passthrough\")\n",
    "        X = transform.fit_transform(X)\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Data has been successfully One Hot Encoded\")\n",
    "        print(\"#########################################################################\")\n",
    "        return X\n",
    "        \n",
    "    else:\n",
    "        return X #return data with no changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def riskEncodingData(X,dataOption):#Risk encoding categorical features\n",
    "    #Manually Encoding for the attacks types only\n",
    "    if dataOption == \"3\": #if data option is risk Value\n",
    "        X = pd.DataFrame(X)\n",
    "        servers  = {'http':0.01, 'domain_u':0, 'sunrpc':1, 'smtp':0.01, 'ecr_i':0.87, 'iso_tsap':1, 'private':0.97, 'finger':0.27, 'ftp':0.26, 'telnet':0.48,'other':0.12,'discard':1, 'courier':1, 'pop_3':0.53, 'ldap':1, 'eco_i':0.8, 'ftp_data':0.06, 'klogin':1, 'auth':0.31, 'mtp':1, 'name':1, 'netbios_ns':1,'remote_job':1,'supdup':1,'uucp_path':1,'Z39_50':1,'csnet_ns':1,'uucp':1,'netbios_dgm':1,'urp_i':0,'domain':0.96,'bgp':1,'gopher':1,'vmnet':1,'systat':1,'http_443':1,'efs':1,'whois':1,'imap4':1,'echo':1,'link':1,'login':1,'kshell':1,'sql_net':1,'time':0.88,'hostnames':1,'exec':1,'ntp_u':0,'nntp':1,'ctf':1,'ssh':1,'daytime':1,'shell':1,'netstat':1,'nnsp':1,'IRC':0,'pop_2':1,'printer':1,'tim_i':0.33,'pm_dump':1,'red_i':0,'netbios_ssn':1,'rje':1,'X11':0.04,'urh_i':0,'http_8001':1,'aol':1,'http_2784':1,'tftp_u':0,'harvest':1}\n",
    "        X[2] = [servers[item] for item in X[2]]\n",
    "\n",
    "        servers_Error  = {'REJ':0.519, 'SF':0.016, 'S0':0.998, 'RSTR':0.882, 'RSTO':0.886,'SH':0.993,'S1':0.008,'RSTOS0':1,'S3':0.08,'S2':0.05,'OTH':0.729} \n",
    "        X[3] = [servers_Error[item] for item in X[3]]\n",
    "\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Data has been successfully risk Encoded\")\n",
    "        print(\"#########################################################################\")\n",
    "\n",
    "        return X\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        return X #return data with no changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(X):#Scalign the data with the normalize method, we scale the data to have it in the same range for the experiments\n",
    "    \n",
    "    \n",
    "\n",
    "    while True:\n",
    "            \n",
    "            decision = input(\"Scale data [y/n]:\")\n",
    "            \n",
    "            if decision == \"y\" or  decision == \"n\":\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "                print(\"Error\\n\\n\")\n",
    "    \n",
    "    if decision == \"y\":\n",
    "        \n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            #Transforms features by scaling each feature to a given range.\n",
    "            X =  MinMaxScaler(feature_range=(0, 1)).fit_transform(X)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Data has been successfully scaled.\")\n",
    "            print(\"#########################################################################\")\n",
    "            return X\n",
    "        \n",
    "    else:\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleData(X):# currently a bug, if we do shuffleling the experiments resutls are not good, the order of the data does not affect the results\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "    while True:\n",
    "        option = input(\"Shuffle data [y]/[n]:\")\n",
    "        \n",
    "        if option == \"y\" or option == \"n\":\n",
    "            break\n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "    \n",
    "    if option == \"y\":\n",
    "        \n",
    "        X = pd.DataFrame(X)\n",
    "        X = shuffle(X)\n",
    "        X.reset_index(inplace=True,drop=True)\n",
    "        X = np.array(X)\n",
    "        \n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Data has been successfully shuffled.\")\n",
    "        print(\"#########################################################################\")\n",
    "        return X\n",
    "    else:\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeansClustering(X,Y):#K-means algorithm \n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"KMEANS ALGORITHM\")\n",
    "        print(\"#########################################################################\")\n",
    "              \n",
    "        nClusters = input(\"Number of clusters:\")\n",
    "        \n",
    "        try:\n",
    "            nClusters = int(nClusters)\n",
    "            \n",
    "        except ValueError:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "            \n",
    "        if type(nClusters) == int:\n",
    "            n = 0\n",
    "            clusters = []\n",
    "            \n",
    "            while n < nClusters:#Converting nCluster into an array of n clusters [n] for use it later\n",
    "                clusters.append(n)\n",
    "                n+=1\n",
    "            break\n",
    "        \n",
    "    while True:\n",
    "        init = input(\"Initialization method [k-means++,random]:\")\n",
    "        \n",
    "        if init == \"k-means++\" or init == \"random\":\n",
    "            break\n",
    "\n",
    "    print(\"\\nClustering...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    KMEANS = KMeans(n_clusters = nClusters, init = init,max_iter = 300,n_init = 10,random_state = 0)\n",
    "    print(\"\\n\\nRun Time ->\",\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"Data Successfully Clustered\")\n",
    "    kmeans = KMEANS.fit(X)\n",
    "    Z = kmeans.labels_\n",
    "    inertia = KMEANS.inertia_\n",
    "    #Kmeans Results\n",
    "    kmeansR = pd.crosstab(Y,Z)\n",
    "    maxVal = kmeansR.idxmax()\n",
    "    \n",
    "    return Z,clusters,kmeansR,maxVal,inertia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kF1(Z,Y,maxVal,clusters):#F1 Score for Kmeans\n",
    "    from sklearn.metrics import f1_score\n",
    "    #Encoding data to F-score\n",
    "    \n",
    "    \n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    n = 0 # counter\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "    f1 = 0 #f1score\n",
    "    average = ''\n",
    "    \n",
    "    while n < len(clusters):# while counter < number of clusters\n",
    "        dictionaryCluster[clusters[n]] = maxVal[n] #creating key(cluster index) with value (max number of the clustering results) for every iteration\n",
    "        n+=1\n",
    "        \n",
    "    Z[:] = [dictionaryCluster[item] for item in Z[:]] # match key with the index of klabels and replace it with key value\n",
    "            \n",
    "    Y = np.array(Y,dtype = int) # Converting labels into a int array\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        average = input(\"Average Method[weighted,micro,macro,binary]:\")\n",
    "        \n",
    "        if average == \"weighted\" or average == \"micro\" or average == \"macro\" or average == 'binary':\n",
    "            break\n",
    "    #score metric   \n",
    "    f1 = f1_score(Y,Z, average = average) #Forget the labels that where not predicted and gives lables that were predicted at least once\n",
    "    \n",
    "    return f1,dictionaryCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMEANS Normal Mutial Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kNMI(Z,Y,maxVal,clusters):\n",
    "    from sklearn.metrics import normalized_mutual_info_score\n",
    "    \n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    n = 0 # counter\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "    NMI = 0\n",
    "    average = ''\n",
    "    \n",
    "    while n < len(clusters):# while counter < number of clusters\n",
    "        dictionaryCluster[clusters[n]] = maxVal[n] #creating key(cluster index) with value (max number of the clustering results) for every iteration\n",
    "        n+=1\n",
    "        \n",
    "    Z[:] = [dictionaryCluster[item] for item in Z[:]] # match key with the index of klabels and replace it with key value\n",
    "    \n",
    "    Y = np.array(Y,dtype = int) #Making sure that labels are in a int array\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        average = input(\"Average Method[geometric,min,arithmetic,max]:\")\n",
    "        \n",
    "        if average == \"geometric\" or average == \"min\" or average == \"arithmetic\" or average == \"max\":\n",
    "            break\n",
    "    #Score metric \n",
    "    NMI = normalized_mutual_info_score(Y, Z, average_method = average)\n",
    "    \n",
    "    return NMI,dictionaryCluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMEANS Adjusted Random Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kARS(Z,Y,maxVal,clusters):\n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "    \n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    n = 0 # counter\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "    ars = 0\n",
    "    \n",
    "    while n < len(clusters):# while counter < number of clusters\n",
    "        dictionaryCluster[clusters[n]] = maxVal[n] #creating key(cluster index) with value (max number of the clustering results) for every iteration\n",
    "        n+=1\n",
    "        \n",
    "    Z[:] = [dictionaryCluster[item] for item in Z[:]] # match key with the index of klabels and replace it with key value\n",
    "    \n",
    "    Y = np.array(Y,dtype = int) #Making sure that labels are in a int array\n",
    "    \n",
    "    #score metric\n",
    "    ars = adjusted_rand_score(Y, Z)\n",
    "    \n",
    "    return ars,dictionaryCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscanClustering(X,Y):#DBSCAN algorithm\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"DBSCAN ALGORITHM\")\n",
    "        print(\"#########################################################################\")\n",
    "              \n",
    "        epsilon = input(\"epsilon[Decimal]:\")\n",
    "        \n",
    "        try:\n",
    "            epsilon = float(epsilon)\n",
    "            \n",
    "        except ValueError:\n",
    "            \n",
    "            print(\"Enter a Decimal number\")\n",
    "            \n",
    "            \n",
    "        if type(epsilon) == float:\n",
    "            break\n",
    "        \n",
    "    while True:\n",
    "        minSamples = input(\"Min Samples[Integer]:\")\n",
    "        \n",
    "        try:\n",
    "            minSamples = int(minSamples)\n",
    "            \n",
    "        except ValueError:\n",
    "            \n",
    "            print(\"Enter a Integer Number\")\n",
    "            \n",
    "        if type(minSamples) == int:\n",
    "            break\n",
    "        \n",
    "    while True:\n",
    "        algorithm = input(\"Algorithm['auto’, ‘ball_tree’, ‘kd_tree’, 'brute']:\")\n",
    "            \n",
    "        if algorithm == \"auto\" or algorithm == \"ball_tree\" or algorithm == \"kd_tree\" or algorithm == \"brute\":\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"Error\\n\\n\")\n",
    "            \n",
    "    \n",
    "    print(\"\\nClustering...\\n\")\n",
    "\n",
    "    #Compute DBSCAN\n",
    "    start_time = time.time() \n",
    "    db = DBSCAN(eps= epsilon, min_samples = minSamples,algorithm = algorithm).fit(X)\n",
    "    print(\"\\n\\nRun Time ->\",\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"Data Successfully Clustered\")\n",
    "    \n",
    "    \n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    \n",
    "    Z = db.labels_\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters = len(set(Z))\n",
    "    n_noise_ = list(Z).count(-1)\n",
    "    \n",
    "    n = -1  # DBSCAN return index -1 cluster\n",
    "    clusters = []\n",
    "    while n + 1 < n_clusters:\n",
    "        clusters.append(n)\n",
    "        n += 1\n",
    "    \n",
    "    #DBSCAN Results\n",
    "    dbscanR = pd.crosstab(Y,Z)\n",
    "    maxVal = dbscanR.idxmax()\n",
    "    \n",
    "    return Z,clusters,n_noise_,dbscanR,maxVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbF1(Z,Y,clusters,maxVal):#F1 score for DBSCAN\n",
    "    from sklearn.metrics import f1_score\n",
    "    #Encoding data to F-score\n",
    "    \n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    n = 0 # counter\n",
    "    c = -1 # - counter max Value has negative index\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "    f1 = 0\n",
    "    average = ''\n",
    "    \n",
    "    while n < len(clusters):# while counter < number of clusters\n",
    "        dictionaryCluster[clusters[n]] = maxVal[c] #creating key(cluster index) with value (max number of the clustering results) for every iteration\n",
    "        n+=1\n",
    "        c+=1\n",
    "    \n",
    "        \n",
    "    Z[:] = [dictionaryCluster[item] for item in Z[:]] # match key with the index of klabels and replace it with key value\n",
    "    \n",
    "    Y = np.array(Y,dtype = int) #Making sure that labels are in a int array\n",
    "    while True:\n",
    "        \n",
    "        average = input(\"Average Method[weighted,micro,macro]:\")\n",
    "        \n",
    "        if average == \"weighted\" or average == \"micro\" or average == \"macro\":\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "    #score metric\n",
    "    f1 = f1_score(Y,Z, average = average)\n",
    "    return f1,dictionaryCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN Mutual Info Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbNMI(Z,Y,clusters,maxVal):# Mutual info score for dbscan\n",
    "    from sklearn.metrics import normalized_mutual_info_score\n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    n = 0 # counter\n",
    "    c = -1 # - counter max Value has negative index\n",
    "    NMI = 0\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "    average = ''\n",
    "    \n",
    "    while n < len(clusters):# while counter < number of clusters\n",
    "        dictionaryCluster[clusters[n]] = maxVal[c] #creating key(cluster index) with value (max number of the clustering results) for every iteration\n",
    "        n+=1\n",
    "        c+=1\n",
    "    \n",
    "    Y = np.array(Y,dtype = int) #Making sure that labels are in a int array\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        average = input(\"Average Method[geometric,min,arithmetic,max]:\")\n",
    "        \n",
    "        if average == \"geometric\" or average == \"min\" or average == \"arithmetic\" or average == \"max\":\n",
    "            break\n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "    #score metric\n",
    "    NMI = normalized_mutual_info_score(Y, Z, average_method= average)\n",
    "    \n",
    "    return NMI,dictionaryCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN Adjusted Random Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbARS(Z,Y,clusters,maxVal): # adjusted rand score for dbscan\n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "    \n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    n = 0 # counter\n",
    "    c = -1 # - counter max Value has negative index\n",
    "    ars = 0\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "    \n",
    "    while n < len(clusters):# while counter < number of clusters\n",
    "        dictionaryCluster[clusters[n]] = maxVal[c] #creating key(cluster index) with value (max number of the clustering results) for every iteration\n",
    "        n+=1\n",
    "        c+=1\n",
    "    #score metric\n",
    "    ars = adjusted_rand_score(Y,Z)\n",
    "    \n",
    "    return ars,dictionaryCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolationForest(X,Y):# isolation forest algorithm\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    \n",
    "    while True:\n",
    "        contamination = input(\"Contamination[Float 0 to 0.5]: \")\n",
    "        \n",
    "        try:\n",
    "            contamination = float(contamination)\n",
    "            \n",
    "        except ValueError:\n",
    "            \n",
    "            print(\"Enter a Number\")\n",
    "            \n",
    "        if type(contamination) == float and (contamination >= 0 and contamination <= 0.5):\n",
    "            break\n",
    "    \n",
    "    print(\"\\nClustering...\\n\")   \n",
    "    \n",
    "    start_time = time.time() \n",
    "    Z = IsolationForest(max_samples = \"auto\",behaviour = \"new\",contamination = contamination).fit_predict(X)\n",
    "    print(\"\\n\\nRun Time ->\",\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    Z = np.array(Z,dtype = object)\n",
    "    \n",
    "    ifR = pd.crosstab(Y,Z)\n",
    "    ifR = pd.DataFrame(ifR)\n",
    "    maxVal = ifR.idxmax()\n",
    "    \n",
    "    n = -1  # Isolation Forest return index -1 and 1 cluster\n",
    "    clusters = []\n",
    "    while n < len(ifR.columns):\n",
    "        clusters.append(n)\n",
    "        n += 2\n",
    "        \n",
    "    return Z,ifR,maxVal,clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifF1(Z,Y,clusters,maxVal): #f1 score for isolation forest\n",
    "    from sklearn.metrics import f1_score\n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    \n",
    "    n = 0 # counter\n",
    "    c = -1 # - counter max Value has negative index\n",
    "    f1 = 0\n",
    "    average = ''\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "\n",
    "    \n",
    "    while n < len(clusters): # Since we got -1 and 1 clusters , in order to assing the corrects result counter starts at -1 and it increments by 2 so it can have the 1 index of maxLOFvalue\n",
    "        dictionaryCluster[clusters[n]] = maxVal[c] \n",
    "        n+=1\n",
    "        c+=2\n",
    "        \n",
    "    Z[:] = [dictionaryCluster[item] for item in Z[:]] # match key with the index of klabels and replace it with key value\n",
    "    \n",
    "    Y = np.array(Y,dtype = int)\n",
    "    Z = np.array(Z,dtype = int)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        average = input(\"Average Method[weighted,micro,macro]:\")\n",
    "        \n",
    "        if average == \"weighted\" or average == \"micro\" or average == \"macro\":\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "    # score metric\n",
    "    f1 = f1_score(Y,Z, average = average) #[None, 'micro', 'macro', 'weighted']\n",
    "    \n",
    "    return f1,dictionaryCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOF(X,Y):# Local outlier factor algorithm\n",
    "    from sklearn.neighbors import LocalOutlierFactor \n",
    "    \n",
    "    while True:\n",
    "        contamination = input(\"Contamination[Float 0 to 0.5]: \")\n",
    "        \n",
    "        try:\n",
    "            contamination = float(contamination)\n",
    "            \n",
    "        except ValueError:\n",
    "            \n",
    "            print(\"Enter a Number\")\n",
    "            \n",
    "        if type(contamination) == float and (contamination > 0 and contamination <= 0.5):\n",
    "            break\n",
    "        \n",
    "    while True:\n",
    "        algorithm = input(\"Algorithm['auto’, ‘ball_tree’, ‘kd_tree’, 'brute']:\")\n",
    "            \n",
    "        if algorithm == \"auto\" or algorithm == \"ball_tree\" or algorithm == \"kd_tree\" or algorithm == \"brute\":\n",
    "            break\n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "            \n",
    "    print(\"\\nClustering...\\n\")\n",
    "    \n",
    "    start_time = time.time() \n",
    "    lof = LocalOutlierFactor(contamination = contamination,algorithm = algorithm).fit_predict(X)\n",
    "    print(\"\\n\\nRun Time ->\",\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    lofR = pd.crosstab(Y,lof)\n",
    "    maxVal = lofR.idxmax()\n",
    "    \n",
    "    \n",
    "    n = -1  # LOF return index -1 and 1 cluster\n",
    "    clusters = []\n",
    "    while n < len(lofR.columns):\n",
    "        clusters.append(n)\n",
    "        n += 2\n",
    "    \n",
    "    \n",
    "    \n",
    "    return lof,lofR,maxVal,clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Outlier Factor F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lofF1(Z,Y,clusters,maxVal): # f1 score for local outlier factor\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    # This part of the code automatically assign the max-ocurring instance in each found cluster to that specific found cluster,in order to evaluate the clustering with greater ease.\n",
    "    n = 0 # counter\n",
    "    c = -1 # - counter max Value has negative index\n",
    "    f1 = 0\n",
    "    dictionaryCluster  = {} # creating an empty dictionary \n",
    "    \n",
    "    while n < len(clusters): # Since we got -1 and 1 clusters , in order to assing the corrects result counter starts at -1 and it increments by 2 so it can have the 1 index of maxLOFvalue\n",
    "        dictionaryCluster[clusters[n]] = maxVal[c] \n",
    "        n+=1\n",
    "        c+=2\n",
    "        \n",
    "    Z[:] = [dictionaryCluster[item] for item in Z[:]] # match key with the index of klabels and replace it with key value\n",
    "    Y = np.array(Y,dtype = int)\n",
    "    Z = np.array(Z,dtype = int)\n",
    "    while True:\n",
    "        \n",
    "        average = input(\"Average Method[weighted,None,micro,macro]:\")\n",
    "        \n",
    "        if average == \"weighted\" or average == \"micro\" or average == \"macro\" or average == \"None\":\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "    f1 = f1_score(Y,Z, average = average) #[None, 'micro', 'macro', 'weighted']\n",
    "    \n",
    "    return f1,dictionaryCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear()\n",
    "#Calling the functions\n",
    "\n",
    "##########################################################################\n",
    "path,dataSetOption = getDataSet()\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "dataSet = readingData(path)\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "dataSet = checkMissing(dataSet)\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "data,labels,dataOption = gettingVariables(dataSet,dataSetOption) #Getting the Data we want to use for the algorithms\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "try:\n",
    "    labels,encodeOption = encodingLabels(labels,dataOption,dataSetOption) #Encoding the true labels\n",
    "except ValueError:\n",
    "    labels = encodingLabels(labels,dataOption,dataSetOption) #Encoding the true labels\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "data = riskEncodingData(data,dataOption)\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "data = oneHotEncodingData(data,dataOption) #One hot Encode with the complete data\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "data = scaling(data)\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "data = shuffleData(data)\n",
    "#########################################################################\n",
    "\n",
    "#This menu is a option to run diferrent algorithms with the same preproceced data witouth the need of running all the code from 0 to make another experiment.\n",
    "while True:  \n",
    "    while True:\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Algorithm Menu\")\n",
    "        print(\"#########################################################################\")\n",
    "        \n",
    "        print(\"1.Kmeans\")\n",
    "        print(\"2.Dbscan\")\n",
    "        print(\"3.Isolation Forest\")\n",
    "        print(\"4.Local Factor Outlier\")\n",
    "        \n",
    "        algorithmOption = input(\"option:\")\n",
    "        \n",
    "        if algorithmOption == \"1\" or algorithmOption == \"2\" or algorithmOption == \"3\" or algorithmOption == \"4\":\n",
    "                break\n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "\n",
    "    \n",
    "    if algorithmOption == \"1\":\n",
    "        #########################################################################\n",
    "        #KMEANS\n",
    "        klabels,kClusters,kmeansR,maxKvalue,inertia = kmeansClustering(data,labels)\n",
    "        print(\"#########################################################################\")\n",
    "        print(\"KMEANS RESULTS\\n\\n\")\n",
    "        print(\"Clusters -> \",kClusters,\"\\n\")\n",
    "        print(\"Inertia -> \",inertia)\n",
    "        print(kmeansR,\"\\n\\n\")\n",
    "        print(\"Max True Label\",\"\\n\\n\",maxKvalue)\n",
    "        print(\"#########################################################################\")\n",
    "        #########################################################################\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Kmeans Score Metrics Menu\")\n",
    "        print(\"#########################################################################\")\n",
    "        \n",
    "        while True:\n",
    "            print(\"1.F1 Score\")\n",
    "            print(\"2.Normalized Mutual Info Score\")\n",
    "            print(\"3.Adjusted Rand Score\")\n",
    "        \n",
    "            kScoreOption = input(\"option:\")\n",
    "            \n",
    "            if kScoreOption == \"1\" or kScoreOption == \"2\" or kScoreOption == \"3\":\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "                print(\"Error\\n\\n\")\n",
    "     \n",
    "        if kScoreOption == \"1\":\n",
    "            #########################################################################\n",
    "            #F1 Score\n",
    "            kmeansF1,clusterAssigned = kF1(klabels,labels,maxKvalue,kClusters)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"KMEANS F1 Score -> \",kmeansF1)\n",
    "            print(\"#########################################################################\")\n",
    "            #########################################################################\n",
    "        \n",
    "        elif kScoreOption == \"2\":\n",
    "            #########################################################################\n",
    "            kmeansNMI,clusterAssigned = kNMI(klabels,labels,maxKvalue,kClusters)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"KMEANS Normalized Mutual Info Score -> \",kmeansNMI)\n",
    "            print(\"#########################################################################\")\n",
    "            #########################################################################\n",
    "    \n",
    "        elif kScoreOption == \"3\":\n",
    "            \n",
    "            #########################################################################\n",
    "            kmeansARS,clusterAssigned = kARS(klabels,labels,maxKvalue,kClusters)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"KMEANS Adjusted Rand Score -> \",kmeansARS)\n",
    "            print(\"#########################################################################\")\n",
    "            #########################################################################\n",
    "            \n",
    "    elif algorithmOption == \"2\":\n",
    "        #########################################################################\n",
    "        #DBSCAN\n",
    "        dblabels,dbClusters,nNoises,dbscanR,maxDBvalue = dbscanClustering(data,labels) \n",
    "        print(\"#########################################################################\")\n",
    "        print(\"DBSCAN RESULTS\\n\\n\")\n",
    "        print(\"Clusters -> \",dbClusters,\"\\n\")\n",
    "        print(dbscanR,\"\\n\\n\")\n",
    "        print(\"Noise -> \",nNoises)\n",
    "        print(\"Max True Label\",\"\\n\\n\",maxDBvalue)\n",
    "        print(\"#########################################################################\")\n",
    "        #########################################################################\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Dscan Score Metrics Menu\")\n",
    "        print(\"#########################################################################\")\n",
    "        print(\"1.F1 Score\")\n",
    "        print(\"2.Normalized Mutual Info Score\")\n",
    "        print(\"3.Adjusted Rand Score\")\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            dbScoreOption = input(\"option:\")\n",
    "            \n",
    "            if dbScoreOption == \"1\" or dbScoreOption == \"2\" or dbScoreOption == \"3\":\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "                print(\"Error\\n\\n\")\n",
    "    \n",
    "        if dbScoreOption == \"1\":\n",
    "            #########################################################################\n",
    "            #F1 Score dbscan\n",
    "            dbscanF1,clusterAssigned = dbF1(dblabels,labels,dbClusters,maxDBvalue)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"DBSCAN F1 Score -> \",dbscanF1)\n",
    "            print(\"#########################################################################\")\n",
    "            #########################################################################\n",
    "            \n",
    "        elif dbScoreOption == \"2\":\n",
    "            #########################################################################\n",
    "            dbscanNMI,clusterAssigned = dbNMI(dblabels,labels,dbClusters,maxDBvalue)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"DBSCAN Normalized Mutual Info Score -> \",dbscanNMI)\n",
    "            print(\"#########################################################################\")\n",
    "            #########################################################################\n",
    "            \n",
    "        elif dbScoreOption == \"3\":\n",
    "            #########################################################################\n",
    "            dbscanARS,clusterAssigned = dbARS(dblabels,labels,dbClusters,maxDBvalue)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"DBSCAN Adjusted Rand Score -> \",dbscanARS)\n",
    "            print(\"#########################################################################\")\n",
    "            #########################################################################\n",
    "        \n",
    "        \n",
    "    elif algorithmOption == \"3\":\n",
    "        #########################################################################\n",
    "        ifLabels,ifR,MaxIfVal,ifNclusters = isolationForest(data,labels)\n",
    "        print(\"#########################################################################\")\n",
    "        print(\"Isolation Forest RESULTS\\n\\n\")\n",
    "        print(\"Clusters -> \",ifNclusters,\"\\n\")\n",
    "        print(ifR,\"\\n\\n\")\n",
    "        print(\"Max True Label\",\"\\n\\n\",MaxIfVal)\n",
    "        print(\"#########################################################################\")\n",
    "        #########################################################################\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"Isolation Forest Score Metrics Menu\")\n",
    "        print(\"#########################################################################\")\n",
    "        print(\"1.F1 Score\")\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            ifScoreOption = input(\"option:\")\n",
    "            \n",
    "            if ifScoreOption == \"1\":\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "                print(\"Error\\n\\n\")\n",
    "        \n",
    "        if ifScoreOption == \"1\":\n",
    "            \n",
    "            ##########################################################################\n",
    "            isolationForestF1,clusterAssigned = ifF1(ifLabels,labels,ifNclusters,MaxIfVal)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"Isolation Forest F1 Score -> \",isolationForestF1)\n",
    "            print(\"#########################################################################\")\n",
    "            ##########################################################################\n",
    "        \n",
    "    elif algorithmOption == \"4\":\n",
    "        #########################################################################\n",
    "        LOFlabels,lofR,maxLOFvalue,lofClusters = LOF(data,labels)\n",
    "        print(\"#########################################################################\")\n",
    "        print(\"Local Outlier Factor RESULTS\\n\\n\")\n",
    "        print(\"Clusters -> \",lofClusters,\"\\n\")\n",
    "        print(lofR,\"\\n\\n\")\n",
    "        print(\"Max True Label\",\"\\n\\n\",maxLOFvalue)\n",
    "        print(\"#########################################################################\")\n",
    "        #########################################################################\n",
    "        print(\"\\n\\n#########################################################################\")\n",
    "        print(\"LOF Score Metrics Menu\")\n",
    "        print(\"#########################################################################\")\n",
    "        print(\"1.F1 Score\")\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            lofScoreOption = input(\"option:\")\n",
    "            \n",
    "            if lofScoreOption == \"1\":\n",
    "                break\n",
    "            else:\n",
    "                \n",
    "                print(\"Error\\n\\n\")\n",
    "        \n",
    "        if lofScoreOption == \"1\":\n",
    "            \n",
    "            ##########################################################################\n",
    "            LOFf1,clusterAssigned = lofF1(LOFlabels,labels,lofClusters,maxLOFvalue)\n",
    "            print(\"\\n\\n#########################################################################\")\n",
    "            print(\"Cluster Matchings by Maximun Intersection[Found: True] -> \",clusterAssigned)\n",
    "            print(\"LOF F1 Score -> \",LOFf1)\n",
    "            print(\"#########################################################################\")\n",
    "            ##########################################################################\n",
    "                \n",
    "    while True: # If the user want to Make a new clustering algorithm test\n",
    "        \n",
    "        decision = input(\"Try another Clustering Algorithm[y/n]:\")\n",
    "        \n",
    "        if decision == \"y\" or  decision == \"n\":\n",
    "            break\n",
    "        else:\n",
    "            \n",
    "            print(\"Error\\n\\n\")\n",
    "    \n",
    "    \n",
    "    if decision == \"n\":\n",
    "        break\n",
    "    \n",
    "    else:\n",
    "        clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "923px",
    "left": "328px",
    "right": "20px",
    "top": "9px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
